---
title: May 20th, 2020
---

## Strategy stealing. Christiano references "space of possibilities" that's shared between agents, but it seems like there are holes in this. For instance, if current day Longtermists find a way to acausal trade outside the universe, I don't think others will consider that possible for a long time afterwards, both
### because its effects are unobservable

### because it requires fundamental changes in worldview

### https://ai-alignment.com/the-strategy-stealing-assumption-a26b8b1ed334

### If an aligned AI defines its values with reference to “whatever Paul wants,” then someone doesn’t need to kill Paul to mess with the AI, they just need to change what Paul wants.
#### This doesn't seem in line with AIA thought. The unaligned ai wouldn't want exactly what paul wants, and it may be locked in to his preferences at its creation, not continuously

## I have this intuition that optimal is not what I want but something that "someone like me with more imagination would want"

## This could theoretically be a problem without AI, e.g. a large group of human with shared explicit values might be able to coordinate better and so leave normal humans at a disadvantage, though I think this is relatively unlikely as a major force in the world.
### This is just EA, and it's true we don't own the world

## On generalization

## Humans think we can generalize because we have a strong framework in which most things make sense. We think "humans know what a human is," but that breaks down in the cases of simulated humans, or uplifted animals or whatever.

## I think [[Directed Exploration]] is the most important aspect of agi that needs to be invented

## [[AI Alignment Newsletter]] 100
### Combining these results with the increased available compute over time, the authors estimate that the effective training compute available to the largest AI experiments has increased by a factor of 7.5 million (!) in 2018 relative to 2012. 
#### wtf

## Model distillation (using a larger trained neural network to train a smaller neural network) reduces OOD robustness, suggesting that naive in-distribution tests for model distillation methods may mask later failures.
### why? Idk
