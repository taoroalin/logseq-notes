---
title: analysis of papers in [[AIAN]] from a [[humans-as-gears]] perspective
---

## [[AN #124]]
### https://arxiv.org/abs/2009.12612 Neuro Symbolic Shielding
#### This uses gradient descent to train symbolic policies, and then prove those policies are safe. If this is scaled up to humans-as-gears, it would in theory still work, because its constraints are proven, not based on human input to something black boxy

### https://arxiv.org/abs/2010.11645 what is [[Semidefinite Programming]]
#### it's fancy maths stuff I don't want to learn right now

### 

## [[Iterated Amplification and Distillation]]
### here's the argument (from memory): Imagine you have a "pure" optimiser, that has no systemic bias, but does have limited power and noise. Let's say you have an algorithm to combine multiple of these into a "conglomeration" thing that's more capable than individuals. Let's say you can "distil" this conglomeration into one [[AI]] that has the elevated performance of the conglomeration but more heat efficiency. Do this lots of times until have agi. 

### How can the amplification step be made pure? Humans working in a team are nowhere close to purely preserving values. 
